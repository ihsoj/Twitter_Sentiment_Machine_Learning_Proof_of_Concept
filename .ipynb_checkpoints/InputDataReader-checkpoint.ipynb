{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                       is so sad for my APL frie...\n",
       "1       2          0                     I missed the New Moon trail...\n",
       "2       3          1                            omg its already 7:30 :O\n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data into dataframe\n",
    "file = os.path.join('..', 'Raw Data', 'train.csv' )\n",
    "tweet_df = pd.read_csv(file, encoding='latin-1')\n",
    "tweet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tweet length\n",
    "tweet_df['pre_clean_len'] = [len(t) for t in tweet_df.SentimentText]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'partial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-b1bb6a58b856>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mslang_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslang_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# longest first for regex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mregex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\b({})\\b\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslang_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mreplaceSlang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslang_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# remove re-tweets (i.e., RT)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'partial' is not defined"
     ]
    }
   ],
   "source": [
    "# clean html tags\n",
    "import string  \n",
    "import html\n",
    "import re\n",
    "from \n",
    "\n",
    "## Added by Dean\n",
    "# remove Account Handlers (such as @username)\n",
    "tweet_df['pre_clean_txt'] = [re.sub('@[^\\s]+','',text) for text in tweet_df.SentimentText]\n",
    "\n",
    "# # # Remove slang words \n",
    "# with open('slang.txt') as file:\n",
    "#     slang_map = dict(map(str.strip, line.partition('\\t')[::2])\n",
    "#     for line in file if line.strip())\n",
    "\n",
    "# slang_words = sorted(slang_map, key=len, reverse=True) # longest first for regex\n",
    "# regex = re.compile(r\"\\b({})\\b\".format(\"|\".join(map(re.escape, slang_words))))\n",
    "# replaceSlang = partial(regex.sub, lambda m: slang_map[m.group(1)])\n",
    "\n",
    "# remove re-tweets (i.e., RT)\n",
    "tweet_df['pre_clean_txt'] = [text.replace(u\"rt\", \"\") for text in tweet_df.pre_clean_txt]\n",
    "\n",
    "# remove url\n",
    "tweet_df['pre_clean_txt'] = [re.sub(r'http\\S+', '', t) for t in tweet_df.pre_clean_txt]\n",
    "\n",
    "# unescape html characters\n",
    "tweet_df['pre_clean_txt'] = [html.unescape(t) for t in tweet_df.pre_clean_txt]\n",
    "\n",
    "# remove string.punctiation: !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "tweet_df['pre_clean_txt'] = [t.translate(str.maketrans('', '', string.punctuation)) for t in tweet_df.pre_clean_txt]\n",
    "\n",
    "# remove numbers\n",
    "tweet_df['pre_clean_txt'] = [t.translate(str.maketrans('', '', string.digits)) for t in tweet_df.pre_clean_txt]\n",
    "\n",
    "# to lower case\n",
    "tweet_df['pre_clean_txt'] = [t.lower() for t in tweet_df.pre_clean_txt]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace consequent characters, match words against dictionary\n",
    "#from pattern.en import suggest\n",
    "\n",
    "def reduce_lengthening(text):\n",
    "   pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "   return pattern.sub(r\"\\1\\1\", text)\n",
    "\n",
    "# def get_clean_word(word):\n",
    "#     word_wlf = suggest(reduce_lengthening(word))[0]\n",
    "#     if(word_wlf[1] > 0.5):\n",
    "#         return word_wlf[0]\n",
    "#     return word\n",
    "\n",
    "# remove repetitive charecters\n",
    "#tweet_df['pre_clean_txt'] = [get_clean_word(t) for t in tweet_df.pre_clean_txt]\n",
    "tweet_df['pre_clean_txt'] = [reduce_lengthening(t) for t in tweet_df.pre_clean_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "#set(stopwords.words('english'))\n",
    "\n",
    "# Dean added this line to tokenize the words\n",
    "tweet_df['tokenized'] = tweet_df['pre_clean_txt'].apply(nltk.word_tokenize)\n",
    "\n",
    "\n",
    "# Dean added this line to remove the stop words\n",
    "tweet_df['tokenized'] = tweet_df['tokenized'].apply(lambda x: [item for item in x if item not in stop_words])\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#initializes porter\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "\n",
    "# for row in tweet_df.rows():\n",
    "#     tweet_df.loc[row, 'stemmed'] = [porter.stem(t) for t in tweet_df.loc[row, 'tokenized']]\n",
    "\n",
    "for i in tweet_df.index:\n",
    "    tweet_df.at[i, 'tokenized'] = [porter.stem(t) for t in tweet_df.at[i, 'tokenized']]\n",
    "    tweet_df.at[i, 'clean_txt'] = TreebankWordDetokenizer().detokenize(tweet_df.at[i, 'tokenized'])\n",
    "\n",
    "\n",
    "#tweet_df = tweet_df.drop(['tokenized', 'ItemID'], axis =1)\n",
    "tweet_df.head(10)\n",
    "\n",
    "file = os.path.join('..', 'Output', 'cleanData.csv' )\n",
    "tweet_df.to_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dictionary of words\n",
    "import re \n",
    "words_df = pd.DataFrame(tweet_df['pre_clean_txt'].values.tolist(), index=tweet_df.index)\n",
    "\n",
    "word_dict = {}\n",
    "\n",
    "for ww in words_df[0]:\n",
    "    for w in re.findall(r'\\w+', ww):\n",
    "        if w in word_dict: \n",
    "            word_dict[w] += 1\n",
    "        else: \n",
    "            word_dict[w] = 1\n",
    "\n",
    "# wc = [t for t in word_dict]\n",
    "\n",
    "words_df1 = pd.DataFrame.from_dict(word_dict, orient='index')\n",
    "\n",
    "file = os.path.join('..', 'Output', 'words.csv' )\n",
    "#wfile = open(file,\"w\")\n",
    "#wfile.writelines(sorted(word_dict.keys())\n",
    "#wfile.close()\n",
    "    \n",
    "#sorted(word_dict.keys())\n",
    "words_df1.head(100)\n",
    "words_df1.to_csv(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 36 (PythonData36)",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
